## Overview

This is a portfolio website that includes a chatbot that answers user questions using content from your pages.
Although this was implemented as a porfolio, the chatbot will work for any kind of website (blogs, documentation, etc)
if configured properly.
The only thing that needs to change, outside the instructions in Getting Started, is the LLM's system prompt (`prompt`
variable) in [route.ts](./src/app/api/chat/route.ts) (Line 62).
You can also change the embedding model used in [astradb.ts](./src/lib/astradb.ts) (Line 3, 17).

## Setting Up

After cloning the repository make sure you set up your environment variables in an .env.local file. You can refer to
the [.env.example](./.env.example) file for the required environment variables.

### ASTRA DB

This implementation uses AstraDB for storing the text embeddings generated by the LLM. You can sign up for a free tier
account at [DataStax](https://astra.datastax.com) and create a database.
Follow these steps after creating an account:

1. Click on the Databases tab and select Create Database.
2. Choose the Serverless (Vector) option and select a region, provider, and database name.
3. Paste the API Endpoint under the overview section into the .env.local file as `ASTRA_DB_ENDPOINT`.
4. Select generate token under Application Tokens and paste the token into the .env.local file as
   `ASTRA_DB_APPLICATION_TOKEN`.
5. When the database is done initializing, click the Data Explorer Tab and create an empty collection.
6. Your collection name will be the value of `ASTRA_DB_COLLECTION` in the .env.local file.

If you use a different name from the collection you created in the environment variable Astra will create a new
collection using the name in you .env file.

### OLLAMA

```shell
npm install
```

For this implementation, langchain uses Ollama but you can use other providers by changing the model in the [
`route.ts`](./src/app/api/chat/route.ts) (Line 8, 29, 36).
This setup is for the Ollama model so you will need to install Ollama and start it. You don't need the model running
while the using the chatbot; just ensure the Ollama service is running on your device.
Visit [ollama.com](https://ollama.com/) to install ollama, then choose an available model.
This implementation used llama3.2 but you can use any model. The output from reasoning models might show `<think>` in
their output as that has not been handled yet.

For Linux (valid for Mac and Windows too), after installing Ollama, start the Ollama service by running:

```shell
ollama serve
```

Mac and Windows users can alternatively open the Ollama application if the ollama serve command fails. Then install your
preferred models using `ollama pull <model_name>`.
For this implementation, you will want to run (in a different terminal window from `ollama serve`):

```shell
ollama pull llama3.2
ollama pull mxbai-embed-large
```

The dimensions of the collection are determined by the embedding length of the embedding model used. If you used a
different model, be sure to change the dimensions in [astradb.ts](./src/lib/astradb.ts) (Line 24).
After the dimensions are set and you generate the embeddings, you cannot change the dimensions without creating a new
collection else you'll get an error.  
For example, in this implementation, the embedding model used was mxbai-embed-large which has an embedding length of
1024 so the dimensions are set to 1024 in [astradb.ts](./src/lib/astradb.ts) (Line 17, under `collectionOptions`).
If you use nomic-embed-text which has an embedding length of 768, you would change the dimensions to 768.
To check the embedding length of a model, you can run `ollama show <model_name>` in the terminal.
To check the embedding length of mxbai-embed-large, for example, you would run:

```bash
ollama show mxbai-embed-large
```

If the operation is successful you should see "success" after each pull command. Run:

```shell
npm run build && npm run start
```

Building automatically generates the text embeddings for the chatbot. Text embeddings are generated in
the [generate.ts](./scripts/generate.ts) file and stored in AstraDB. The script goes through all files named `page.tsx`
in the [app](./src/app) directory and generates embeddings using the text in those pages. The current implementation
reads only text so it cannot read images or other files. The url of the page is also stored to help the chatbot direct
users to the page the information was obtained from or whenever it is relevant to (as per the system prompt). This means
you don't have to modify the generate.ts file to add new pages. Just create a new page in the app directory and the
script will generate the embeddings for it.
